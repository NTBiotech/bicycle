{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27491fbc-7598-41af-8358-5277b39a28c1",
   "metadata": {},
   "source": [
    "# Small usage demo on Norman dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e41fc-e1d3-4e65-b71e-eae0431d5b76",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b32525-d1b7-4875-9659-bf435867de00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Norman et al. (https://doi.org/10.1126/science.aax4438) dataset from\n",
    "# scPerturb (http://projects.sanderlab.org/scperturb/, https://zenodo.org/records/7041849)\n",
    "\n",
    "#!wget https://zenodo.org/records/7041849/files/NormanWeissman2019_filtered.h5ad?download=1\n",
    "#!mv NormanWeissman2019_filtered.h5ad?download=1 NormanWeissman2019_filtered.h5ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cb8bd-8168-404c-8ab5-ce874f9637ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c4ba5-a754-42ff-85c7-a4b107740fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad('NormanWeissman2019_filtered.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a02ec-8661-4fed-b587-5cf11f089f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just for demo purposes: Reduce number of genes only to genes, on which we have observed perturbations\n",
    "# (One could also add highly variable genes or genes according to some other selection criterion)\n",
    "\n",
    "all_perturbations = adata.obs.perturbation.unique()\n",
    "target_genes = [gene for gene in all_perturbations if ( (not '_' in gene) and gene != 'control')]\n",
    "\n",
    "print(target_genes)\n",
    "\n",
    "# Reduce the set of target genes to 20 for faster debugging\n",
    "# FIXME: Remove this line\n",
    "target_genes = target_genes[:20]\n",
    "\n",
    "adata = adata[:,adata.var.index.isin(target_genes)]\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e7b93-2ef6-4036-98da-aac7addccde8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select cells, for which the perturbed genes were also observed / sequenced\n",
    "\n",
    "conditions = adata.obs['perturbation'].unique()\n",
    "\n",
    "ok_conditions = set()\n",
    "\n",
    "for cond in conditions:\n",
    "    \n",
    "    if cond == 'control' or (cond in adata.var.index) or ('_' in cond and cond.split('_')[0] in adata.var.index and cond.split('_')[1] in adata.var.index):\n",
    "        ok_conditions.add(cond)\n",
    "        \n",
    "adata = adata[adata.obs.perturbation.isin(ok_conditions)]\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fb68d-c904-49b9-bfa3-2d999cf92d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "ok_conditions = list(ok_conditions)\n",
    "\n",
    "n_conditions = len(ok_conditions)\n",
    "n_genes = len(adata.var.index)\n",
    "\n",
    "cond_map = dict()\n",
    "\n",
    "gt_interv = torch.tensor(np.zeros( (n_genes, n_conditions) )).long()\n",
    "\n",
    "for i, cond in enumerate(ok_conditions):\n",
    "    \n",
    "    cond_map[cond] = i\n",
    "    \n",
    "    for j, gene in enumerate(adata.var.index):\n",
    "        \n",
    "        if gene in cond:\n",
    "            gt_interv[j, i] = 1\n",
    "\n",
    "# Let us have a look, how such a matrix looks like :)            \n",
    "plt.figure(figsize = (40,20))            \n",
    "sns.heatmap(gt_interv, yticklabels = adata.var.index, xticklabels = ok_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ab168-38e1-4c7a-aba2-f5a1b2b9d501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create samples matrix\n",
    "\n",
    "samples = torch.tensor(np.asarray(adata.X.todense())).float()\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930baad8-bd3f-4b5a-bc09-09c7ba429c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create regime vector\n",
    "\n",
    "regimes = torch.tensor(np.asarray([cond_map[pert] for pert in adata.obs.perturbation])).long()\n",
    "regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a6926-82dc-43f5-ad4e-448707516075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now just create a train-validation-test split\n",
    "\n",
    "# Use single-gene perturbations for training and validation and put \n",
    "\n",
    "train_gene_ko = [str(x) for x in range(n_genes)]\n",
    "\n",
    "# Use double-gene perturbations for testing\n",
    "\n",
    "# Regimes for training (and validation/hyperparameter tuning)\n",
    "train_regimes = list()\n",
    "\n",
    "# Regimes with dual perturbations to hold out for testing\n",
    "test_regimes = list()\n",
    "\n",
    "for c in range(n_conditions):\n",
    "    if gt_interv[:,c].sum() > 1.5:\n",
    "        test_regimes.append(c)\n",
    "    else:\n",
    "        train_regimes.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000548e2-97e2-491a-b4e2-0bba34c3e396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bicycle.utils.data import create_loaders_norman\n",
    "\n",
    "# Generate data loaders\n",
    "\n",
    "validation_size = 0.2\n",
    "batch_size = 10240\n",
    "SEED = 0\n",
    "\n",
    "train_loader, validation_loader, test_loader = create_loaders_norman(\n",
    "    samples,\n",
    "    regimes,\n",
    "    validation_size,\n",
    "    batch_size,\n",
    "    SEED,\n",
    "    train_regimes,\n",
    "    test_regimes\n",
    ")\n",
    "\n",
    "covariates = None\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(f\"- Number of training samples: {len(train_loader.dataset)}\")\n",
    "print(\"Training regimes:\", train_regimes)\n",
    "if validation_size > 0:\n",
    "    print(f\"- Number of validation samples: {len(validation_loader.dataset)}\")\n",
    "if len(test_regimes) > 0:\n",
    "    print(f\"- Number of test samples: {len(test_loader.dataset)}\")\n",
    "    print(\"Test regimes:\", test_regimes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a5c8a-2cc3-42fb-a4e8-7714aa8e928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os import environ\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from bicycle.dictlogger import DictLogger\n",
    "from bicycle.model import BICYCLE\n",
    "from bicycle.utils.data import (\n",
    "    create_loaders,\n",
    "    get_diagonal_mask,\n",
    ")\n",
    "from bicycle.utils.general import get_full_name\n",
    "from bicycle.utils.plotting import plot_training_results\n",
    "from pytorch_lightning.callbacks import RichProgressBar, StochasticWeightAveraging\n",
    "from bicycle.callbacks import ModelCheckpoint, GenerateCallback, MyLoggerCallback, CustomModelCheckpoint\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "n_factors = 0\n",
    "\n",
    "# The Norman data have CRISPRa interventions - use dCas9\n",
    "intervention_type_inference = \"dCas9\"\n",
    "\n",
    "SEED = 1\n",
    "pl.seed_everything(SEED)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "user_dir = \".\"\n",
    "\n",
    "MODEL_PATH = Path(os.path.join(user_dir, \"models\"))\n",
    "PLOT_PATH = Path(os.path.join(user_dir, \"plots\"))\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#\n",
    "# Settings\n",
    "#\n",
    "\n",
    "# TRAINING\n",
    "lr = 1e-3\n",
    "USE_INITS = False\n",
    "use_encoder = False\n",
    "n_epochs = 51000\n",
    "early_stopping = False\n",
    "early_stopping_patience = 500\n",
    "early_stopping_min_delta = 0.01\n",
    "# Maybe this helps to stop the loss from growing late during training (see current version\n",
    "# of Plot_Diagnostics.ipynb)\n",
    "optimizer = \"adam\" #\"rmsprop\" #\"adam\"\n",
    "optimizer_kwargs = {}\n",
    "#    \"betas\": [0.5,0.9] # Faster decay for estimates of gradient and gradient squared\n",
    "#}\n",
    "gradient_clip_val = 1e-3\n",
    "GPU_DEVICE = 0\n",
    "plot_epoch_callback = 500\n",
    "validation_size = 0.2\n",
    "lyapunov_penalty = True\n",
    "swa = 250\n",
    "n_epochs_pretrain_latents = 1000#10000\n",
    "\n",
    "# MODEL\n",
    "x_distribution = \"Multinomial\"\n",
    "x_distribution_kwargs = {}\n",
    "model_T = 1.0\n",
    "learn_T = False\n",
    "use_latents = True\n",
    "perfect_interventions = True\n",
    "rank_w_cov_factor = n_genes # Fitting full covariance matrices for multivariate normals\n",
    "\n",
    "# RESULTS\n",
    "name_prefix = f\"2TEST_CHECKPOINTS_Norman_Demo_optim{optimizer}_b1_0.5_b2_0.9_pretrain_epochs{n_epochs_pretrain_latents}_GRAD-CLIP_INF:{intervention_type_inference}-slow_lr_{use_encoder}_{batch_size}_{lyapunov_penalty}\"\n",
    "SAVE_PLOT = True\n",
    "CHECKPOINTING = True\n",
    "VERBOSE_CHECKPOINTING = True\n",
    "OVERWRITE = False\n",
    "# REST\n",
    "n_samples_total = len(adata)\n",
    "check_val_every_n_epoch = 1\n",
    "log_every_n_steps = 1\n",
    "\n",
    "# Create Mask\n",
    "mask = get_diagonal_mask(n_genes, device)\n",
    "\n",
    "if n_factors > 0:\n",
    "    mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb8b85-9210-46cc-a831-ffd4105a288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_INITS:\n",
    "    init_tensors = compute_inits(train_loader.dataset, rank_w_cov_factor, n_contexts)\n",
    "\n",
    "device = torch.device(f\"cuda:{GPU_DEVICE}\")\n",
    "gt_interv = gt_interv.to(device)\n",
    "n_genes = samples.shape[1]\n",
    "\n",
    "if covariates is not None and correct_covariates:\n",
    "    covariates = covariates.to(device)\n",
    "\n",
    "for scale_kl in [1.0]:  # 1\n",
    "    for scale_l1 in [1.0]:\n",
    "        for scale_spectral in [0.0]: # 1.0\n",
    "            for scale_lyapunov in [0.1]: # 0.1\n",
    "                file_dir = get_full_name(\n",
    "                    name_prefix,\n",
    "                    len(test_regimes),\n",
    "                    SEED,\n",
    "                    lr,\n",
    "                    n_genes,\n",
    "                    scale_l1,\n",
    "                    scale_kl,\n",
    "                    scale_spectral,\n",
    "                    scale_lyapunov,\n",
    "                    gradient_clip_val,\n",
    "                    swa,\n",
    "                )\n",
    "\n",
    "                # If final plot or final model exists: do not overwrite by default\n",
    "                print(\"Checking Model and Plot files...\")\n",
    "                final_file_name = os.path.join(MODEL_PATH, file_dir, \"last.ckpt\")\n",
    "                final_plot_name = os.path.join(PLOT_PATH, file_dir, \"last.png\")\n",
    "                \n",
    "                # Save simulated data for inspection and debugging\n",
    "                final_data_path = os.path.join(PLOT_PATH, file_dir)\n",
    "                \n",
    "                if os.path.isdir(final_data_path):\n",
    "                    print(final_data_path, \"exists\")\n",
    "                else:\n",
    "                    print(\"Creating\", final_data_path)\n",
    "                    os.mkdir(final_data_path)\n",
    "                \n",
    "                np.save(os.path.join(final_data_path,'check_samples.npy'), samples.detach().cpu().numpy())\n",
    "                np.save(os.path.join(final_data_path,'check_regimes.npy'), regimes.detach().cpu().numpy())\n",
    "                np.save(os.path.join(final_data_path,'check_gt_interv.npy'), gt_interv.detach().cpu().numpy())\n",
    "                \n",
    "                labels = list(adata.var.index)\n",
    "                \n",
    "                np.save(os.path.join(final_data_path,'labels.npy'), labels, allow_pickle=True)\n",
    "                \n",
    "                if (Path(final_file_name).exists() & SAVE_PLOT & ~OVERWRITE) | (\n",
    "                    Path(final_plot_name).exists() & CHECKPOINTING & ~OVERWRITE\n",
    "                ):\n",
    "                    print(\"- Files already exists, skipping...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"- Not all files exist, fitting model...\")\n",
    "                    print(\"  - Deleting dirs\")\n",
    "                    # Delete directories of files\n",
    "                    if Path(final_file_name).exists():\n",
    "                        print(f\"  - Deleting {final_file_name}\")\n",
    "                        # Delete all files in os.path.join(MODEL_PATH, file_name)\n",
    "                        for f in os.listdir(os.path.join(MODEL_PATH, file_dir)):\n",
    "                            os.remove(os.path.join(MODEL_PATH, file_dir, f))\n",
    "                    if Path(final_plot_name).exists():\n",
    "                        print(f\"  - Deleting {final_plot_name}\")\n",
    "                        for f in os.listdir(os.path.join(PLOT_PATH, file_dir)):\n",
    "                            os.remove(os.path.join(PLOT_PATH, file_dir, f))\n",
    "\n",
    "                    print(\"  - Creating dirs\")\n",
    "                    # Create directories\n",
    "                    Path(os.path.join(MODEL_PATH, file_dir)).mkdir(parents=True, exist_ok=True)\n",
    "                    Path(os.path.join(PLOT_PATH, file_dir)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                model = BICYCLE(\n",
    "                    lr,\n",
    "                    gt_interv,\n",
    "                    n_genes,\n",
    "                    n_samples=n_samples_total,\n",
    "                    lyapunov_penalty=lyapunov_penalty,\n",
    "                    perfect_interventions=perfect_interventions,\n",
    "                    rank_w_cov_factor=rank_w_cov_factor,\n",
    "                    init_tensors=init_tensors if USE_INITS else None,\n",
    "                    optimizer=optimizer,\n",
    "                    optimizer_kwargs = optimizer_kwargs,\n",
    "                    device=device,\n",
    "                    scale_l1=scale_l1,\n",
    "                    scale_lyapunov=scale_lyapunov,\n",
    "                    scale_spectral=scale_spectral,\n",
    "                    scale_kl=scale_kl,\n",
    "                    early_stopping=early_stopping,\n",
    "                    early_stopping_min_delta=early_stopping_min_delta,\n",
    "                    early_stopping_patience=early_stopping_patience,\n",
    "                    early_stopping_p_mode=True,\n",
    "                    x_distribution=x_distribution,\n",
    "                    x_distribution_kwargs=x_distribution_kwargs,\n",
    "                    mask=mask,\n",
    "                    use_encoder=use_encoder,\n",
    "                    train_gene_ko=train_regimes,\n",
    "                    test_gene_ko=test_regimes,\n",
    "                    use_latents=use_latents,\n",
    "                    covariates=covariates,\n",
    "                    n_factors = n_factors,\n",
    "                    intervention_type = intervention_type_inference,\n",
    "                    T = model_T,\n",
    "                    learn_T = learn_T\n",
    "                )\n",
    "                model.to(device)\n",
    "\n",
    "                dlogger = DictLogger()\n",
    "                loggers = [dlogger]\n",
    "\n",
    "                callbacks = [\n",
    "                    RichProgressBar(refresh_rate=1),\n",
    "                    GenerateCallback(\n",
    "                        final_plot_name, plot_epoch_callback=plot_epoch_callback,labels=labels\n",
    "                    ),\n",
    "                ]\n",
    "                if swa > 0:\n",
    "                    callbacks.append(StochasticWeightAveraging(0.01, swa_epoch_start=swa))\n",
    "                if CHECKPOINTING:\n",
    "                    Path(os.path.join(MODEL_PATH, file_dir)).mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    print('Checkpointing to:',MODEL_PATH)\n",
    "                    \n",
    "                    callbacks.append(\n",
    "                        CustomModelCheckpoint(\n",
    "                            dirpath=os.path.join(MODEL_PATH, file_dir),\n",
    "                            filename=\"{epoch}\",\n",
    "                            save_last=True,\n",
    "                            save_top_k=1,\n",
    "                            verbose=VERBOSE_CHECKPOINTING,\n",
    "                            monitor=\"valid_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_weights_only=True,\n",
    "                            start_after=1000,\n",
    "                            save_on_train_epoch_end=True,\n",
    "                            every_n_epochs=500,\n",
    "                        )\n",
    "                    )\n",
    "                    callbacks.append(MyLoggerCallback(dirpath=os.path.join(MODEL_PATH, file_dir)))\n",
    "\n",
    "                trainer = pl.Trainer(\n",
    "                    max_epochs=n_epochs,\n",
    "                    accelerator=\"gpu\",  # if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                    logger=loggers,\n",
    "                    log_every_n_steps=log_every_n_steps,\n",
    "                    enable_model_summary=True,\n",
    "                    enable_progress_bar=True,\n",
    "                    enable_checkpointing=CHECKPOINTING,\n",
    "                    check_val_every_n_epoch=check_val_every_n_epoch,\n",
    "                    devices=[GPU_DEVICE],  # if str(device).startswith(\"cuda\") else 1,\n",
    "                    num_sanity_val_steps=0,\n",
    "                    callbacks=callbacks,\n",
    "                    gradient_clip_val=gradient_clip_val,\n",
    "                    default_root_dir=str(MODEL_PATH),\n",
    "                    gradient_clip_algorithm=\"value\",\n",
    "                    deterministic=False, #\"warn\",\n",
    "                )\n",
    "                \n",
    "                '''print('Optimizing learning rates')\n",
    "                \n",
    "                tuner = Tuner(trainer)\n",
    "\n",
    "                # Run learning rate finder\n",
    "                lr_finder = tuner.lr_find(model)\n",
    "\n",
    "                # Results can be found in\n",
    "                print(lr_finder.results)\n",
    "\n",
    "                # Plot with\n",
    "                fig = lr_finder.plot(suggest=True)\n",
    "                fig.save('lr_finder.png')\n",
    "\n",
    "                # Pick point based on plot, or get suggestion\n",
    "                new_lr = lr_finder.suggestion()\n",
    "                \n",
    "                print('Using learning rate of:',new_lr)\n",
    "\n",
    "                # update hparams of the model\n",
    "                model.hparams.lr = new_lr'''\n",
    "\n",
    "                \n",
    "                if use_latents and n_epochs_pretrain_latents > 0:\n",
    "                    \n",
    "                    pretrain_callbacks = [\n",
    "                        RichProgressBar(refresh_rate=1),\n",
    "                        GenerateCallback(\n",
    "                            str(Path(final_plot_name).with_suffix(\"\")) + '_pretrain', plot_epoch_callback=plot_epoch_callback,labels=labels\n",
    "                        ),                    \n",
    "                    ]\n",
    "                    \n",
    "                    if swa > 0:\n",
    "                        pretrain_callbacks.append(StochasticWeightAveraging(0.01, swa_epoch_start=swa))\n",
    "    \n",
    "                    pretrain_callbacks.append(MyLoggerCallback(dirpath=os.path.join(MODEL_PATH, file_dir)))\n",
    "                    \n",
    "                    pretrainer = pl.Trainer(\n",
    "                        max_epochs=n_epochs_pretrain_latents,\n",
    "                        accelerator=\"gpu\",  # if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                        logger=loggers,\n",
    "                        log_every_n_steps=log_every_n_steps,\n",
    "                        enable_model_summary=True,\n",
    "                        enable_progress_bar=True,\n",
    "                        enable_checkpointing=CHECKPOINTING,\n",
    "                        check_val_every_n_epoch=check_val_every_n_epoch,\n",
    "                        devices=[GPU_DEVICE],  # if str(device).startswith(\"cuda\") else 1,\n",
    "                        num_sanity_val_steps=0,\n",
    "                        callbacks=pretrain_callbacks,\n",
    "                        gradient_clip_val=gradient_clip_val,\n",
    "                        default_root_dir=str(MODEL_PATH),\n",
    "                        gradient_clip_algorithm=\"value\",\n",
    "                        deterministic=False, #\"warn\",\n",
    "                    )\n",
    "                    \n",
    "                    print('PRETRAINING LATENTS!')\n",
    "                    start_time = time.time()\n",
    "                    model.train_only_likelihood = True\n",
    "                    # assert False\n",
    "                    pretrainer.fit(model, train_loader, validation_loader)\n",
    "                    end_time = time.time()\n",
    "                    model.train_only_likelihood = False\n",
    "                \n",
    "                # try:\n",
    "                start_time = time.time()\n",
    "                # assert False\n",
    "                trainer.fit(model, train_loader, validation_loader)\n",
    "                end_time = time.time()\n",
    "                print(f\"Training took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "                plot_training_results(\n",
    "                    trainer,\n",
    "                    model,\n",
    "                    model.beta.detach().cpu().numpy(),\n",
    "                    beta,\n",
    "                    scale_l1,\n",
    "                    scale_kl,\n",
    "                    scale_spectral,\n",
    "                    scale_lyapunov,\n",
    "                    final_plot_name,\n",
    "                    callback=False,\n",
    "                )\n",
    "                # except Exception as e:\n",
    "                #     # Write Exception to file\n",
    "                #     report_path = os.path.join(MODEL_PATH, file_dir, \"report.yaml\")\n",
    "                #     # Write yaml\n",
    "                #     with open(report_path, \"w\") as outfile:\n",
    "                #         yaml.dump({\"exception\": str(e)}, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301beb2-f54e-448d-98ed-0a1aac3bc18a",
   "metadata": {},
   "source": [
    "# Plots, logs and beta-estimates will be written to the ./plots/RUN_NAME subfolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd70bf-b438-485c-bf70-2ef1808690a0",
   "metadata": {},
   "source": [
    "# Plot loss curves and examine GRN matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fa95c-4a15-4b00-80a2-127b89989b36",
   "metadata": {
    "tags": []
   },
   "source": [
    "These log files used in the following cells are written every `plot_epoch_callback` during training, so they can also be used to monitor convergende during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f836f-327b-4367-9e78-d3ddb0f308c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "base_path = \"/g/stegle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919f6cf-2f7e-4b75-94fe-b131989ceffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gseapy as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661253c-51f7-438e-baaa-8a493e11f9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_vars_from_df(df, variables):\n",
    "    plt.figure()\n",
    "\n",
    "    for variable in variables:\n",
    "        df_plot = df[df[\"variable\"] == variable]\n",
    "        plt.plot( df_plot[\"epoch\"], df_plot[\"value\"], label = variable)\n",
    "\n",
    "    plt.legend()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d4cb4-e88d-454c-9f62-1b9058000154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_path = \"ueltzhoe/bicycle_main/bicycle/notebooks/experiments/norman/plots/Norman_Demo_optimadam_b1_0.5_b2_0.9_pretrain_epochs1000_GRAD-CLIP_INF:dCas9-slow_lr_False_10240_True_8_1_0.001_20_1.0_1.0_0.0_0.1_0.001_250/last_log_train.csv\"\n",
    "df = pd.read_csv(os.path.join(base_path,log_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b49e10-8cab-47b3-a5e5-05202260d787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.variable.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21e205-66ef-4890-a266-6aa06ffea07d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "variables = [\n",
    "    \"train_l1\",\n",
    "    \"train_nll_train\",\n",
    "    \"train_kl_train\",\n",
    "    \"train_loss\"\n",
    "]\n",
    "\n",
    "for var in variables:\n",
    "    plot_vars_from_df(df, [var])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "270d171f-2bec-4387-b062-1c5629a6d979",
   "metadata": {
    "tags": []
   },
   "source": [
    "variables = [\n",
    "    \"train_cov_weight_min\",\n",
    "    \"train_cov_weight_mean\",\n",
    "    \"train_cov_weight_max\"\n",
    "]\n",
    "\n",
    "plot_vars_from_df(df, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996eddba-2403-4342-ad29-55f76f1ad210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "gene_list_path = 'ueltzhoe/bicycle_main/bicycle/notebooks/experiments/norman/plots/Norman_Demo_optimadam_b1_0.5_b2_0.9_pretrain_epochs1000_GRAD-CLIP_INF:dCas9-slow_lr_False_10240_True_8_1_0.001_20_1.0_1.0_0.0_0.1_0.001_250/labels.npy'\n",
    "\n",
    "gene_list = np.load(os.path.join(base_path,gene_list_path), allow_pickle=True)\n",
    "gene_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87cdbd1-46e9-4fb0-8371-0da522772cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "estimated_path = \"ueltzhoe/bicycle_main/bicycle/notebooks/experiments/norman/plots/Norman_Demo_optimadam_b1_0.5_b2_0.9_pretrain_epochs1000_GRAD-CLIP_INF:dCas9-slow_lr_False_10240_True_8_1_0.001_20_1.0_1.0_0.0_0.1_0.001_250/last_estimated_beta_epoch6500.npy\"\n",
    "estimated_beta = np.load(os.path.join(base_path,estimated_path))\n",
    "\n",
    "# Apply threshold (optional)\n",
    "#estimated_beta = np.abs(estimated_beta) > 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b746f-7599-41e0-8155-a16996d676bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Estimated Whole Network\")\n",
    "sns.heatmap(estimated_beta[:,:],\n",
    "            annot = False,\n",
    "            annot_kws={\"fontsize\": 7},\n",
    "            center=0,\n",
    "            cmap=\"vlag\",\n",
    "            vmin = -1.0,\n",
    "            vmax = 1.0\n",
    "           )\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Distribution of edge strengths')\n",
    "plt.hist(estimated_beta.flatten(), range = [-1.0,1.0], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791839bc-5e79-4a5e-9d56-f6c6162f582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER: \\beta_ij is the LINEAR EFFECT OF THE EXPRESSION OF GENE i \n",
    "#           on the TRANSCRIPTION RATE OF GENE j\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Showing x[:,10] = 1 and x[90,:] = 1\")\n",
    "x = np.zeros((100,100))\n",
    "x[:,10] = 1\n",
    "x[90,:] = 1\n",
    "sns.heatmap(x,\n",
    "            annot = False,\n",
    "            annot_kws={\"fontsize\": 7},\n",
    "            center=0,\n",
    "            cmap=\"vlag\",\n",
    "            vmin = -1.0,\n",
    "            vmax = 1.0\n",
    "           )\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a2528-b9fe-4051-8bd6-536b1f6bc8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REMEMBER: \\beta_ij is the LINEAR EFFECT OF THE EXPRESSION OF GENE i \n",
    "#           on the TRANSCRIPTION RATE OF GENE j\n",
    "\n",
    "print('TOTAL SUM:',np.abs(estimated_beta).sum())\n",
    "\n",
    "parents = np.abs(estimated_beta).sum(axis = 0)\n",
    "genes_sorted_by_parents = [{gene_list[i]: parents[i]} for i in np.argsort(-np.abs(parents)).squeeze().astype(int).tolist()]\n",
    "\n",
    "children = np.abs(estimated_beta).sum(axis = 1)\n",
    "genes_sorted_by_children = [{gene_list[i]: children[i]} for i in np.argsort(-np.abs(children)).squeeze().astype(int).tolist()]\n",
    "\n",
    "degree = np.abs(estimated_beta).sum(axis = 1) + np.abs(estimated_beta).sum(axis = 0)\n",
    "genes_sorted_by_degree = [{gene_list[i]: degree[i]} for i in np.argsort(-np.abs(degree)).squeeze().astype(int).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e1642-a1ed-46f0-918a-5702178970af",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_sorted_by_degree[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656399c-a86b-49b8-8f64-54ae019ca6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_sorted_by_parents[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70da53-1f66-401b-ac3c-3b13d2d367e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "genes_sorted_by_children[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb01c31-4e08-4101-a5df-0004588e1ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum_thresh = 1.5\n",
    "category = parents # parents or children\n",
    "\n",
    "nodes_list = [gene_list[i] for i,p in enumerate(parents) if abs(p) > sum_thresh ]               \n",
    "             \n",
    "\n",
    "print(len(nodes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cc542-ad78-4310-97a0-a29c1fe499ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you are only intrested in dataframe that enrichr returned, please set outdir=None\n",
    "enr = gp.enrichr(gene_list=nodes_list, # or \"./tests/data/gene_list.txt\",\n",
    "                 gene_sets=['MSigDB_Hallmark_2020','KEGG_2021_Human'],#'MSigDB_Hallmark_2020','KEGG_2021_Human'],\n",
    "                 organism='human', # don't forget to set organism to the one you desired! e.g. Yeast\n",
    "                 outdir=None, # don't write to disk\n",
    "                )\n",
    "\n",
    "enr.results.groupby('Gene_set').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198ff1d-a892-4209-b0f8-0a0ede20e9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f50f5-7877-4cfc-8273-1457cbb46af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505ece3-a3c7-4d73-a7e1-f73cab0da571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76cd74-1331-46e9-ba2c-0bc211fdece8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df03fe5-8cb7-4700-8e91-1c693acdc62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e98aea30-33ec-4b09-95ca-e058126fc812",
   "metadata": {},
   "source": [
    "# Load model checkpoint and perform evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be334d69-d091-43b7-b2bb-9e55b5399c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"/g/stegle/ueltzhoe/bicycle_main/bicycle/notebooks/experiments/norman/models/DictLogger/0/checkpoints/epoch=624-step=1875.ckpt\"\n",
    "\n",
    "eval_model = BICYCLE.load_from_checkpoint(checkpoint_path=filename, map_location=device, strict=True)\n",
    "\n",
    "# Predicts means for conditions, for which only the alpha_p and sigma_p parameters, modeling the MARGINAL DISTRIBUTION \n",
    "# of the direct perturbation target genes were optimized.\n",
    "\n",
    "# TODO: Implement additional option to either specify or infer/predict these marginal distributions to predict unseen\n",
    "# activation/knock-down/knock-out interventions from similar training interventions on different target genes.\n",
    "\n",
    "eval_model.predict_means(regimes = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6bb82-e908-47cd-9f8b-4ccf425ae09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7c128-9f4a-42df-ac8a-f4f656d06033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18876aa7-5dff-46b8-aae2-aa32ccf6d9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc7633e-eabb-4536-bb82-7e4a27a8b4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23382a1e-1240-4559-bb67-b4414ea2de75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99caea66-2d28-44b6-ac48-8a931b1e3447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681526c-ebcb-4a56-8e7b-7d197999f0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff60c64-f54a-4c01-816e-b1f1211b9437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a9b51-2fcc-43d7-9a50-8966ba979685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d784d-b9db-4f4f-ac6f-514392465e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9118b7-11f5-4faa-9fcf-227e282c93e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981014b-a5a3-40b3-b228-b834f1640b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135544b-c6ce-43b6-89df-78fd08e22005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_bicycle_main",
   "language": "python",
   "name": "env_bicycle_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
